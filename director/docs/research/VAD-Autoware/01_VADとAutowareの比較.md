# VADとAutowareの比較 調査結果報告書<!-- omit in toc -->

## 目次<!-- omit in toc -->

- [はじめに](#はじめに)
- [1. 概要調査：VADとAutowareの基本比較](#1-概要調査vadとautowareの基本比較)
  - [1.1. 目的](#11-目的)
  - [1.2. 参照](#12-参照)
  - [1.3. 概要](#13-概要)
  - [1.4. 特徴比較](#14-特徴比較)
  - [1.5. CARLAシミュレータについて](#15-carlaシミュレータについて)
- [2. VADの詳細分析](#2-vadの詳細分析)
  - [2.1. 概要](#21-概要)
  - [2.2. アーキテクチャ](#22-アーキテクチャ)
  - [2.3. 技術スタック](#23-技術スタック)
  - [2.4. 入出力形式](#24-入出力形式)
  - [2.5. パフォーマンス](#25-パフォーマンス)
- [3. Autowareの詳細分析](#3-autowareの詳細分析)
  - [3.1. 概要](#31-概要)
  - [3.2. アーキテクチャ](#32-アーキテクチャ)
  - [3.3. 技術スタック](#33-技術スタック)
  - [3.4. 入出力形式](#34-入出力形式)
  - [3.5. リポジトリ構成](#35-リポジトリ構成)
- [4. CARLA連携の比較](#4-carla連携の比較)
  - [4.1. VAD × CARLA連携](#41-vad--carla連携)
  - [4.2. Autoware × CARLA連携](#42-autoware--carla連携)
  - [4.3. 連携方式の比較表](#43-連携方式の比較表)
- [5. 技術的課題と考慮事項](#5-技術的課題と考慮事項)
  - [5.1. VAD-CARLA連携の課題](#51-vad-carla連携の課題)
  - [5.2. Autoware-CARLA連携の課題](#52-autoware-carla連携の課題)
- [6. まとめ](#6-まとめ)
  - [6.1. 調査成果](#61-調査成果)
  - [6.2. 技術的知見](#62-技術的知見)
  - [6.3. 推奨使用パターン](#63-推奨使用パターン)

## はじめに

「戦闘力100倍プロジェクト」では、ソフトウェアの進化サイクルのコアとなる「DevOps」、「MLOps」、「不具合解析」に関してAIエージェントを用いることで、通常の100倍ほどの超効率化を目指している。具体的には、ソフト外注で3ヵ月かかることを1日に短縮することを目標とする。

そのうち本プロジェクト（AIエージェント活用自動ソフト開発プロジェクト）では、上記を実現するAIエージェントのフレームワークを試作し、まずは生産性の20〜30倍向上を目標とする。これは、ソフト外注で1ヵ月かかることを1日に短縮することに相当する。

本資料は、ソフトウェア開発の題材としてVAD（Vectorized Scene Representation for Efficient Autonomous Driving）を使用しCARLAと連携させるにあたり、過去に知見があるAutowareとCARLAの連携との違いを明らかにするため調査した結果について記載する。

**調査結論**: VADはエンドツーエンドの深層学習モデルであり、Autowareはモジュラー型の自動運転スタックである。CARLA連携においては、VADはPython APIによる直接統合でセットアップが比較的容易だが、Autowareは ROS 2ブリッジを介した統合で初期セットアップは複雑なものの、モジュール単位でのデバッグが可能という特徴がある。

**本調査の位置づけ**: 本調査結果は、マルチエージェントシステムにおけるMLOpsフェーズでの自動運転モデル開発・検証に活用される。

## 1. 概要調査：VADとAutowareの基本比較

### 1.1. 目的

本調査の目的は、以下を明らかにすることである：

- **VADの特徴把握**: VADの基本的なアーキテクチャ、技術スタック、および特徴を理解
- **Autowareとの違い**: 既知のAutowareと比較した際の根本的なアプローチの違いを明確化
- **CARLA連携方式の違い**: 各システムがCARLAシミュレータとどのように連携するかの違いを把握
- **実装指針の提供**: VAD-CARLA連携を実装する際に必要な知見と考慮事項を整理

### 1.2. 参照

- [VAD: Vectorized Scene Representation for Efficient Autonomous Driving - GitHub](https://github.com/hustvl/VAD)
- [VAD arXiv論文](https://arxiv.org/abs/2303.12077)
- [VADv2 arXiv論文](https://arxiv.org/pdf/2402.13243)
- [Autoware - GitHub](https://github.com/autowarefoundation/autoware)
- [Autoware Documentation](https://autowarefoundation.github.io/autoware-documentation/main/)
- [CARLA Simulator - GitHub](https://github.com/carla-simulator/carla)
- [CARLA Documentation](https://carla.readthedocs.io/en/latest/)
- [Bench2Drive - CARLA実装](https://github.com/Thinklab-SJTU/Bench2Drive)
- [carla-autoware - AutoWare AV stack Bridge](https://github.com/carla-simulator/carla-autoware)

### 1.3. 概要

本調査では、VADとAutowareの2つの自動運転システムについて、アーキテクチャ、技術スタック、CARLA連携方式の観点から比較分析を行った。詳細な比較結果は1.4節以降および6章のまとめを参照のこと。

### 1.4. 特徴比較

VADとAutowareの基本的な特徴を以下の表に比較する。

> ※GitHub Starsの数値は2024年12月時点の値である。最新の情報は各リポジトリを参照のこと。

| 観点 | VAD | Autoware |
|------|-----|----------|
| **タイプ** | エンドツーエンド深層学習モデル | モジュラー型自動運転スタック |
| **開発元** | HUST (Huazhong University of Science and Technology) | Autoware Foundation |
| **公開年** | 2023年 (ICCV 2023) | 2015年から継続的に開発 |
| **リポジトリ** | [hustvl/VAD](https://github.com/hustvl/VAD) | [autowarefoundation/autoware](https://github.com/autowarefoundation/autoware) |
| **ライセンス** | Apache License 2.0 | Apache License 2.0 |
| **GitHub Stars** | 約1,150 | 約10,900 |
| **主要言語** | Python (PyTorch) | C++ (ROS 2) |
| **センシング** | 主にカメラ (マルチビュー) | カメラ、LiDAR、レーダー、GPS/IMU |
| **技術成熟度** | 研究段階 (TRL 3-4) | 実用段階 (TRL 6-8) |

> ※TRL（Technology Readiness Level：技術成熟度レベル）は、技術の成熟度を1〜9の段階で評価する指標である。TRL 3-4は研究開発段階、TRL 6-8は実証・実用化段階を示す。

### 1.5. CARLAシミュレータについて

CARLAは、自動運転研究のためのオープンソースシミュレータである。

> ※GitHub Starsの数値は2024年12月時点の値である。

**基本情報**:
- **リポジトリ**: [carla-simulator/carla](https://github.com/carla-simulator/carla)
- **ライセンス**: MIT License
- **GitHub Stars**: 約13,400
- **主要言語**: C++ (Unreal Engine 4ベース)

**主な特徴**:
- 自動運転システムの開発、訓練、検証をサポート
- オープンソースのコードとプロトコル
- 都市レイアウト、建物、車両などのデジタルアセットを提供
- 柔軟なセンサースイート設定と環境条件の指定が可能
- ROS連携のためのROS-bridgeを提供

---

## 2. VADの詳細分析

### 2.1. 概要

VAD（Vectorized Scene Representation for Efficient Autonomous Driving）は、ICCV 2023で発表されたエンドツーエンドの自動運転モデルである。

**開発元**: Huazhong University of Science and Technology (HUST) と Horizon Robotics の共同研究

**主要な貢献**:
1. ベクトル化されたシーン表現を使用した統一的なエンドツーエンドパラダイムの提案
2. 従来の計算集約的な高密度ラスター表現と手作業による後処理ステップを不要とし、計算効率を向上
3. ベクトル化されたシーン情報を暗黙的・明示的に活用した計画安全性の向上
4. SOTA（State-of-the-Art：最先端）のエンドツーエンド計画性能と高速な推論速度の両立

### 2.2. アーキテクチャ

VADのアーキテクチャは、以下の特徴を持つ：

**エンドツーエンド設計**:
- センサー入力から直接運転制御を生成
- 知覚、予測、計画を単一のニューラルネットワーク内で暗黙的に処理
- クエリ相互作用による計画安全性の向上
- ベクトル化された計画制約の活用

**モデルバリエーション**:

| モデル | バックボーン | 平均L2 | 平均衝突率 | FPS |
| :---: | :---: | :---: | :---: | :---: |
| VAD-Tiny | ResNet-50 | 0.78 | 0.38 | 16.8 |
| VAD-Base | ResNet-50 | 0.72 | 0.22 | 4.5 |

### 2.3. 技術スタック

**プログラミング言語とフレームワーク**:
- Python
- PyTorch

**依存ライブラリ**:
- mmdet3d (OpenMMLab 3D Object Detection)
- detr3d
- BEVFormer
- MapTR

**ハードウェア要件**:
- CUDA対応GPU（訓練・推論用）

### 2.4. 入出力形式

**入力**:
- マルチカメラ画像（6カメラ構成）
- HD Map情報
- 過去の車両状態

**出力**:
- 軌道計画（waypoints）
- 将来の車両位置予測

**データセット形式**:
- nuScenesフォーマットに準拠
- BEV（Bird's Eye View）表現を内部で使用

### 2.5. パフォーマンス

**Open-loop計画結果（nuScenes）**:

| 手法 | L2 (m) 1s | L2 (m) 2s | L2 (m) 3s | 衝突率 (%) 1s | 衝突率 (%) 2s | 衝突率 (%) 3s | FPS |
| :---: | :---: | :---: | :---: | :---:| :---: | :---: | :---: |
| ST-P3 | 1.33 | 2.11 | 2.90 | 0.23 | 0.62 | 1.27 | 1.6 |
| UniAD | 0.48 | 0.96 | 1.65 | 0.05 | 0.17 | 0.71 | 1.8 |
| VAD-Tiny | 0.46 | 0.76 | 1.12 | 0.21 | 0.35 | 0.58 | 16.8 |
| VAD-Base | 0.41 | 0.70 | 1.05 | 0.07 | 0.17 | 0.41 | 4.5 |

**Closed-loopシミュレーション結果（CARLA）**:

| 手法 | Town05 Short DS | Town05 Short RC | Town05 Long DS | Town05 Long RC |
| :---: | :---: | :---: | :---: | :---:|
| CILRS | 7.47 | 13.40 | 3.68 | 7.19 |
| LBC | 30.97 | 55.01 | 7.05 | 32.09 |
| Transfuser* | 54.52 | 78.41 | 33.15 | 56.36 |
| ST-P3 | 55.14 | 86.74 | 11.45 | 83.15 |
| VAD-Base | 64.29 | 87.26 | 30.31 | 75.20 |

> *: LiDARベースの手法

---

## 3. Autowareの詳細分析

### 3.1. 概要

Autowareは、世界をリードするオープンソースの自動運転ソフトウェアプロジェクトである。

**開発元**: Autoware Foundation（産業コンソーシアム）

**目的**:
- Robot Operating System (ROS) 上に構築された自動運転車両用オープンソースソフトウェアスタック
- ローカライゼーションと物体検出からルート計画と制御まで、すべての必要な機能を提供
- できるだけ多くの個人と組織が自動運転技術のオープンイノベーションに貢献できることを目指す

### 3.2. アーキテクチャ

Autowareは、以下のモジュラーアーキテクチャを採用：

**パイプライン構成**:
```
Sensing → Perception → Planning → Control
```

**主要モジュール**:
1. **Sensing**: センサーデータの取得と前処理
2. **Perception**: 物体検出、トラッキング、予測
3. **Planning**: ルート計画、行動計画、軌道生成
4. **Control**: PID/MPC制御器による車両制御

**特徴**:
- 伝統的なロボティクスアプローチと機械学習の組み合わせ
- ROS 2ベースの実装
- 産業グレードの信頼性とテスト可能性
- 実車両への展開を想定した設計

### 3.3. 技術スタック

**プログラミング言語**:
- C++ (コアモジュール)
- Python (ツール、ユーティリティ)

**フレームワーク**:
- ROS 2 (Robot Operating System 2)

**依存コンポーネント**:
- PCL (Point Cloud Library)
- OpenCV
- Eigen
- その他ROS 2エコシステムのパッケージ

### 3.4. 入出力形式

**入力**:
- カメラ画像
- LiDAR点群
- レーダーデータ
- GPS/IMUデータ
- HD Map

**出力**:
- 車両制御コマンド（ステアリング、スロットル、ブレーキ）
- 計画された軌道
- 認識結果（物体検出、追跡）

**通信方式**:
- ROS 2トピック/サービスによるメッセージング
- 標準化されたメッセージ型

### 3.5. リポジトリ構成

Autowareは複数のリポジトリで構成される：

| リポジトリ | 説明 |
|------------|------|
| autoware | メタリポジトリ（.reposファイル含む） |
| autoware_core | 高品質で安定したROS パッケージ |
| autoware_universe | 実験的・最先端のROS パッケージ |
| autoware_launch | ノード設定とパラメータ |
| autoware-documentation | ドキュメンテーション |

---

## 4. CARLA連携の比較

### 4.1. VAD × CARLA連携

**連携の目的**:
- 学習データの生成とモデルの訓練
- エンドツーエンドモデルの評価とベンチマーク

**連携アプローチ**:

1. **データ収集モード**:
   - CARLAから大量の運転データを収集
   - マルチカメラ画像、センサーデータ、ground truthラベルを取得
   - 訓練データセットの生成

2. **評価モード**:
   - 訓練済みVADモデルをCARLA環境で実行
   - モデルの出力（軌道、制御）をCARLA車両に適用
   - パフォーマンスメトリクスの測定

**技術的特徴**:
- **インターフェース**: Python APIを使用した直接的な統合
- **データフロー**: CARLA → センサーデータ → VADモデル → 制御コマンド → CARLA
- **レイテンシ**: リアルタイム性は研究目的のため厳密ではない
- **カスタマイズ**: シナリオベースのテストに重点

**CARLA実装の状況**:

> ※外部ツールの状況は変更される可能性がある。最新情報は各リポジトリを確認のこと。

- VADv1のCARLA実装は[Bench2Drive](https://github.com/Thinklab-SJTU/Bench2Drive)で利用可能（2024年6月公開）

**実装の複雑さ**: 中程度
- VADのPythonコードとCARLA Python APIの統合
- カスタムデータ収集スクリプトが必要
- エンドツーエンドのため、インターフェースポイントが少ない

### 4.2. Autoware × CARLA連携

**連携の目的**:
- 完全な自動運転スタックのシミュレーション検証
- 各モジュールの統合テスト
- 安全性評価と検証

**連携アプローチ（carla-autoware使用）**:

1. **ROSブリッジアーキテクチャ**:
   - CARLAとAutowareの間にブリッジノードを配置
   - センサーデータをROS 2メッセージに変換
   - Autowareの制御コマンドをCARLA車両制御に変換

2. **完全な双方向通信**:
   - CARLA → Autoware: カメラ、LiDAR、GPS/IMU、車両状態
   - Autoware → CARLA: ステアリング、スロットル、ブレーキ、ギア

**技術的特徴**:
- **インターフェース**: ROS 2ブリッジを介した標準化された通信
- **データフロー**:
  ```
  CARLA Sensors → ROS Bridge → Autoware Modules
  Autoware Control → ROS Bridge → CARLA Vehicle
  ```
- **レイテンシ**: リアルタイム性を重視した設計
- **カスタマイズ**: ScenarioRunnerとの統合による複雑なシナリオテスト

**実装の複雑さ**: 高い
- 複数のコンポーネントの統合が必要
  - carla-autoware（またはautoware-carla-interface）のセットアップ
  - ROS 2環境の構築
  - センサー設定のマッピング
  - 座標系の変換
  - タイミング同期
- しかし、一度セットアップすれば各モジュールを個別にテスト可能

### 4.3. 連携方式の比較表

| 観点 | VAD-CARLA | Autoware-CARLA |
|------|-----------|----------------|
| **統合レベル** | アプリケーションレベル | システムレベル |
| **通信方式** | Python API直接呼び出し | ROS 2メッセージング |
| **センサー対応** | 主にカメラ | カメラ、LiDAR、GPS、IMU等 |
| **データ量** | 比較的少ない（画像のみ） | 大量（点群、画像、IMU等） |
| **セットアップ複雑さ** | 中（スクリプトレベル） | 高（システムレベル） |
| **デバッグ性** | 困難（ブラックボックス） | 容易（モジュール単位） |
| **既存ツール** | Bench2Drive | carla-autoware等 |
| **ユースケース** | 研究・学習・ベンチマーク | 検証・テスト・開発 |
| **リアルタイム性** | 必須ではない | 重要 |
| **再現性** | シード管理が重要 | ログ・再生機能が充実 |

---

## 5. 技術的課題と考慮事項

### 5.1. VAD-CARLA連携の課題

**環境構築の課題**:
- PyTorch環境の構築とCUDA依存関係の管理
- mmdet3d等のOpenMMLab依存ライブラリの互換性確保
- CARLAバージョンとの互換性（0.9.13以降推奨）

> ※推奨バージョンは調査時点のものである。最新の互換性情報は[VAD公式リポジトリ](https://github.com/hustvl/VAD)および[CARLA公式ドキュメント](https://carla.readthedocs.io/en/latest/)を参照のこと。

**データ形式の課題**:
- CARLAデータをnuScenesフォーマットに変換する必要性
- マルチカメラキャリブレーション情報の取得と変換
- 時間同期の管理

**評価の課題**:
- エンドツーエンドモデルのブラックボックス性
- デバッグとエラー分析の困難さ
- 中間結果の可視化が限定的

**推奨される実装ステップ**:

> ※以下は推奨される実装ステップの案であり、実際の環境や要件に応じて変更が必要となる場合がある。各ステップの詳細は公式ドキュメントを参照のこと。

1. **環境構築**:
   ```bash
   # VADリポジトリのクローン
   git clone https://github.com/hustvl/VAD

   # 依存関係のインストール
   pip install -r requirements.txt

   # CARLA 0.9.13以降のインストール
   ```

2. **データ収集スクリプトの作成**:
   - CARLAから6カメラ画像を取得
   - 車両状態とground truthを記録
   - nuScenesフォーマットでデータを保存

3. **モデルの訓練**（または事前訓練モデルの使用）:
   - VADモデルの訓練
   - CARLAデータでのファインチューニング

4. **推論ループの実装**:
   ```python
   # 擬似コード
   while simulation_running:
       images = carla_client.get_camera_images()
       trajectory = vad_model.predict(images)
       control = trajectory_to_control(trajectory)
       carla_vehicle.apply_control(control)
   ```

5. **評価**:
   - 成功率、衝突率、快適性等の測定

### 5.2. Autoware-CARLA連携の課題

**システム統合の課題**:
- ROS 2環境のセットアップと依存関係管理
- CARLAとAutowareの通信レイテンシの最適化
- センサー設定のマッピングと座標系変換

**既存の知見で活用可能なスキル**:
- ROS 2のメッセージングシステム
- センサーデータの変換とマッピング
- 車両制御インターフェース
- タイミング同期とレイテンシ管理
- CARLAのシミュレーション環境理解
- シナリオ設定とマップ管理

---

## 6. まとめ

### 6.1. 調査成果

本調査により以下を明らかにした：

1. **VADの特徴**: ベクトル化されたシーン表現を用いたエンドツーエンド自動運転モデルであり、高速な推論速度とSOTA（最先端）性能を両立
2. **Autowareとの根本的違い**: VADは「学習ベース」のエンドツーエンドアプローチ、Autowareは「モジュラー設計」の工学的アプローチという異なる設計思想に基づく
3. **CARLA連携方式の違い**: VADはPython API直接統合、AutowareはROS 2ブリッジ経由という異なる連携方式を採用
4. **実装複雑さの違い**: VADは中程度のセットアップ複雑さ、Autowareは高い初期セットアップコストだが、運用後のモジュール単位でのデバッグ性で優位

### 6.2. 技術的知見

**VAD-CARLA連携で新たに必要となる知識**:
- PyTorchモデルの実行環境構築
- エンドツーエンドモデルの推論パイプライン
- 訓練データの収集と前処理
- ニューラルネットワークの入出力処理
- nuScenesフォーマットの理解

**Autoware経験から転用可能な知識**:
- CARLAシミュレーション環境の理解
- センサー配置と設定
- 評価メトリクスの定義
- シナリオベーステストの手法

### 6.3. 推奨使用パターン

**VADを選択すべき場合**:
- エンドツーエンド学習の研究
- 新しいニューラルアーキテクチャの評価
- ベンチマーク比較
- 高速推論が必要な場合

**Autowareを選択すべき場合**:
- 実用的な自動運転システム開発
- モジュール単位の検証とデバッグ
- 実車両への展開を見据えた開発
- 複数センサー統合が必要な場合

---

[目次](#目次)
